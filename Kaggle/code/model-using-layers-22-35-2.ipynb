{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport glob\nimport PIL.Image as Image\nimport torch.utils.data as data\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-08T21:23:23.764746Z","iopub.execute_input":"2023-06-08T21:23:23.765363Z","iopub.status.idle":"2023-06-08T21:23:23.773072Z","shell.execute_reply.started":"2023-06-08T21:23:23.765301Z","shell.execute_reply":"2023-06-08T21:23:23.771724Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"im1_mean = [0.4042127201928427, 0.412576527722026, 0.42259604676925305, 0.43350413711051056, 0.443855690032959, 0.45140969585826785, 0.45334100553503703, 0.4468871853247354, 0.43043786530179756, 0.404718756014092, 0.37331931684981945, 0.3418279141576013,0.31582148226006085]\nim2_mean = [0.41915108148831637, 0.42160734423110063, 0.423962258067611, 0.42601487354034223, 0.4275087523492551, 0.4281458628877129, 0.4276228290175893, 0.4257433505811504, 0.4224049007987228, 0.41766727063885856, 0.411706916417739, 0.4048067182780746,0.39733730345961626]\nim3_mean = [0.44033469101140527, 0.4503952708250446, 0.46193364562084055, 0.4738397130390685, 0.48420565326379844, 0.49040465635549285, 0.48956380687421636, 0.47946231783341775, 0.45967343462004484, 0.4323601088249301, 0.4019184607310354, 0.37350198569667487,0.3512268327779888]\n\nim1_std = [0.14732023241999434, 0.147809241675677, 0.14804508827283613, 0.1477574222750175, 0.1470682801149269, 0.146539035212084, 0.14675348492637386, 0.14775991989824996, 0.14848621292810835, 0.1465088569090591, 0.13925408874284476, 0.12590440973744257,0.10823487264730804]\nim2_std = [0.12246606074712985, 0.12329658876446659, 0.1242021620079717, 0.12510900974078862, 0.12595538841389356, 0.12670439991891755, 0.12730158828399146, 0.12765988385551708, 0.12760264169433708, 0.1269505535408886, 0.12552824561659184, 0.12318955457930125,0.11984089703152391]\nim3_std = [0.11690682253979083, 0.11736897240085645, 0.11786639327518504, 0.11859482672002022, 0.11987161521520828, 0.1221401983344635, 0.12578272075012656, 0.13025746011922526, 0.13357738663561808, 0.13312490121042017, 0.12725185198165137, 0.11619303606414197,0.10181700640611481]\n\ncalibrator1 = {}\ncalibrator2 = {}\n\nfor i in range(13):\n    layer_idx = i\n    def calibration1(layer_data,layer_idx):\n        mask = (layer_data >= 0.1) & (layer_data <= 0.9) \n        scaled_x = np.copy(layer_data)  \n        scaled_x[mask] = ((layer_data[mask] - im2_mean[layer_idx])/im2_std[layer_idx]) * im1_std[layer_idx] + im1_mean[layer_idx] \n        return scaled_x\n    \n    def calibration2(layer_data,layer_idx):\n        mask = (layer_data >= 0.1) & (layer_data <= 0.9) \n        scaled_x = np.copy(layer_data)  \n        scaled_x[mask] = ((layer_data[mask] - im3_mean[layer_idx])/im3_std[layer_idx]) * im1_std[layer_idx] + im1_mean[layer_idx] \n        return scaled_x    \n\n    calibrator1[layer_idx] = calibration1\n    calibrator2[layer_idx] = calibration2\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:23:24.031393Z","iopub.execute_input":"2023-06-08T21:23:24.032142Z","iopub.status.idle":"2023-06-08T21:23:24.048857Z","shell.execute_reply.started":"2023-06-08T21:23:24.032109Z","shell.execute_reply":"2023-06-08T21:23:24.047908Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numba\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom typing import Tuple, List\nfrom torchvision.transforms import transforms\nimport cv2\n\nclass VesuviusTrainData(Dataset):\n    _relative_sv_dir = \"surface_volume\"  # relative to the directory of train data\n\n    def __init__(self,\n                 dir_path: str = \"train/1\",\n                 z_start: int = 0,\n                 z_end: int = 64,\n                 nucleus_shape: Tuple[int, int] = (16, 16),\n                 hull_size: Tuple[int, int] = (64, 64),\n                 compress_depth =  None,\n                 give_indx = False,\n                 calibrator = None,\n                 boundarysize = None\n                 ):\n        self.dir_path = dir_path\n\n        self.z_start = z_start\n        self.z_end = z_end\n\n        self.nucleus_shape = nucleus_shape\n        self.hull_size = hull_size\n        \n        self.compress_depth = compress_depth\n        self.give_indx = give_indx\n        self.calibrator = calibrator\n        self.boundarysize = boundarysize\n        \n        \n        self._setup()\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, index: int):\n        i, j = self._get_pixel_from_index(self.indices[index])\n        nucleus_height, nucleus_width = self.nucleus_shape\n\n        if self.give_indx == False:\n            if self.calibrator != None:\n                return (torch.from_numpy(\n                    self.calibrate(self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ])),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]))\n            else:\n                return (torch.from_numpy(\n                    self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ]),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height])) \n        else:\n            if self.calibrator != None:\n                return (torch.from_numpy(\n                    self.calibrate(self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ])),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]),\n                   np.stack(([i+k for k in range(nucleus_width)],[\n                    j+k for k in range(nucleus_height)]),axis=1))\n            else:\n                return (torch.from_numpy(\n                    self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ]),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]),\n                   np.stack(([i+k for k in range(nucleus_width)],[\n                    j+k for k in range(nucleus_height)]),axis=1)) \n            \n    def calibrate(self,arr):\n        calibrated_arr = np.zeros_like(arr)\n\n        for i in range(arr.shape[0]):\n            calibration_func = self.calibrator[i]\n            calibrated_arr[i, :, :] = calibration_func(arr[i, :, :],i)\n        return calibrated_arr\n\n\n    def _load_images(self, z_start=None,z_end=None):\n        if z_end is None:\n            z_end = self.z_end \n        if z_start is None:\n            z_start = self.z_start\n            \n        num_images = z_end - z_start\n        self.images = np.empty(\n            (\n                num_images,\n                self.mask.shape[0] + self._left_pad_image + self._right_pad_image,\n                self.mask.shape[1] + self._top_pad_image + self._bottom_pad_image\n            ), \n            dtype=np.float16\n        )\n        for index, i in enumerate(range(z_start, z_end)):\n            # noinspection PyTypeChecker\n            image = np.array(Image.open(f\"{self.dir_path}\"\n                                        f\"/{self._relative_sv_dir}\"\n                                        f\"/{i:02d}.tif\"),\n                             dtype=np.float32) / 65535.0\n            \n            image = image.astype(np.float16)\n            \n            image = np.pad(image, ((self._left_pad_mask + self._left_pad_image,\n                                    self._right_pad_mask + self._right_pad_image),\n                                   (self._top_pad_mask + self._top_pad_image,\n                                    self._bottom_pad_mask + self._bottom_pad_image)),\n                           'constant', constant_values=0)\n            self.images[index, :, :] = image\n            \n    def _load_scale(self):\n        current_stack_layer = self.z_start\n        images = np.empty(\n            (\n                len(self.compress_depth),\n                self.mask.shape[0] + self._left_pad_image + self._right_pad_image,\n                self.mask.shape[1] + self._top_pad_image + self._bottom_pad_image\n            ), \n            dtype=np.float16\n        )\n        for index,layer in enumerate(self.compress_depth):\n            self._load_images(current_stack_layer, current_stack_layer + layer)\n            current_stack_layer += layer\n            layer_stack = self.images\n            images[index,:,:] = np.mean(layer_stack,axis = 0)\n            \n        self.images = images\n        \n        \n    def crop_boundary(self,mask, boundary_size):\n        mask = mask.astype(np.uint8)\n        mask_binary = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY)[1]\n\n        contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        boundary_mask = np.ones_like(mask, dtype=np.uint8)\n\n        cv2.drawContours(boundary_mask, contours, -1, (0), 100)\n\n        kernel_size = int(boundary_size)  # Adjust the kernel size based on your needs\n        kernel = np.ones((kernel_size, kernel_size), dtype=np.uint8)\n        eroded_boundary_mask = cv2.erode(boundary_mask, kernel, iterations=1)\n\n        cropped_mask = mask * eroded_boundary_mask[:, :]\n\n        return cropped_mask\n    \n    # Internal utility methods\n    def _setup(self):\n        # noinspection PyTypeChecker\n        self.mask = np.array(Image.open(f\"{self.dir_path}/mask.png\"))\n        if (self.boundarysize) != None:\n            self.mask = self.crop_boundary(self.mask,self.boundarysize)\n        # noinspection PyTypeChecker\n        self.inklabels = np.array(Image.open(f\"{self.dir_path}/inklabels.png\"))\n        self._granulate_mask()\n        self._fill_new_mask()\n        if self.compress_depth is None:\n            self._load_images()\n        else:\n            self._load_scale()\n        self._get_leftover_hull()\n\n    def _granulate_mask(self):\n        self._get_paddings()\n        self.mask = np.pad(self.mask,\n                           ((self._left_pad_mask, self._right_pad_mask),\n                            (self._top_pad_mask, self._bottom_pad_mask)),\n                           'constant', constant_values=0)\n        self.indices = _find_labeled_nucleus(self.mask, self.nucleus_shape)\n        self.inklabels = np.pad(self.inklabels, \n                                ((self._left_pad_mask, self._right_pad_mask),\n                                 (self._top_pad_mask, self._bottom_pad_mask)),\n                                'constant', \n                                constant_values=0)\n\n    def _get_paddings(self):\n        width, height = self.mask.shape\n        nucleus_width, nucleus_height = self.nucleus_shape\n        hull_width, hull_height = self.hull_size\n        self._left_pad_mask = (width % nucleus_width) // 2\n        self._right_pad_mask = width % nucleus_width - self._left_pad_mask\n        self._top_pad_mask = (height % nucleus_height) // 2\n        self._bottom_pad_mask = height % nucleus_height - self._top_pad_mask\n\n        self._left_pad_image = (hull_width - nucleus_width) // 2\n        self._right_pad_image = hull_width - nucleus_width - self._left_pad_image\n        self._top_pad_image = (hull_height - nucleus_height) // 2\n        self._bottom_pad_image = hull_height - nucleus_height - self._top_pad_image\n\n    def _fill_new_mask(self):\n        nucleus_width, nucleus_height = self.nucleus_shape\n        self.mask = np.zeros_like(self.mask)\n        for index in self.indices:\n            i, j = self._get_pixel_from_index(index)\n            self.mask[\n                i * nucleus_width: (i + 1) * nucleus_width,\n                j * nucleus_height: (j + 1) * nucleus_height\n            ] = 1\n\n    def _get_pixel_from_index(self, index: int):\n        mask_width, mask_height = self.mask.shape\n        nucleus_width, nucleus_height = self.nucleus_shape\n        height = mask_height // nucleus_height\n        return index // height * nucleus_height, index % height * nucleus_width\n\n    def _get_leftover_hull(self):\n        nucleus_width, nucleus_height = self.nucleus_shape\n        hull_width, hull_height = self.hull_size\n        self._left_hull = (hull_width - nucleus_width) // 2\n        self._right_hull = hull_width - nucleus_width - self._left_hull\n        self._top_hull = (hull_height - nucleus_height) // 2\n        self._bottom_hull = hull_height - nucleus_height - self._top_hull\n\n\n@numba.njit()\ndef _find_labeled_nucleus(mask, nuclues_shape):\n    width, height = mask.shape\n    nucleus_width, nucleus_height = nuclues_shape\n\n    indices = - np.ones((width // nucleus_width) * (height // nucleus_height), dtype=np.int32)\n    current_index = 0\n    for i in range(width // nucleus_width):\n        for j in range(height // nucleus_height):\n            if np.sum(\n                    mask[i * nucleus_width: (i + 1) * nucleus_width,\n                         j * nucleus_height: (j + 1) * nucleus_height]\n                      ) > 0:\n                indices[current_index] = i * (height // nucleus_height) + j\n                current_index += 1\n    return indices[indices != -1]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:23:24.793086Z","iopub.execute_input":"2023-06-08T21:23:24.793903Z","iopub.status.idle":"2023-06-08T21:23:26.988696Z","shell.execute_reply.started":"2023-06-08T21:23:24.793869Z","shell.execute_reply":"2023-06-08T21:23:26.987133Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"nucleus_shape = (8,8)\nhull_size = (24,24)\nz_start = 22\nz_end = 35\nboundarysize = 700","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:23:26.991006Z","iopub.execute_input":"2023-06-08T21:23:26.991503Z","iopub.status.idle":"2023-06-08T21:23:26.997012Z","shell.execute_reply.started":"2023-06-08T21:23:26.991453Z","shell.execute_reply":"2023-06-08T21:23:26.996012Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"data1 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/1\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,boundarysize = boundarysize+200)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:23:27.686415Z","iopub.execute_input":"2023-06-08T21:23:27.686868Z","iopub.status.idle":"2023-06-08T21:24:05.924905Z","shell.execute_reply.started":"2023-06-08T21:23:27.686836Z","shell.execute_reply":"2023-06-08T21:24:05.923584Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport cv2\nimport torch.nn.functional as F\nexample_loader = DataLoader(data1,batch_size=1,shuffle=True)\n\ndef zoom_3d_image(image, zoom_factor):\n    # Calculate the target shape based on the zoom factor\n    target_shape = (int(image.shape[0] * zoom_factor),\n                    int(image.shape[1] * zoom_factor),\n                    int(image.shape[2] * zoom_factor))\n    \n    # Perform trilinear interpolation to resize the image\n    zoomed_image = F.interpolate(image.unsqueeze(0).unsqueeze(0), size=target_shape, mode='trilinear')\n    \n    # Upsample the zoomed image to the original size\n    upsampled_image = F.interpolate(zoomed_image, size=image.shape, mode='nearest')\n    \n    # Remove the extra dimensions and return the upsampled image as a PyTorch tensor\n    upsampled_image = upsampled_image.squeeze()\n    \n    return upsampled_image\n\ndef zoom_2d_image(image, zoom_factor):\n    # Calculate the target shape based on the zoom factor\n    target_shape = (int(image.shape[0] * zoom_factor),\n                    int(image.shape[1] * zoom_factor))\n    \n    # Perform bilinear interpolation to resize the image\n    zoomed_image = F.interpolate(image.unsqueeze(0).unsqueeze(0), size=target_shape, mode='bilinear')\n    \n    # Upsample the zoomed image to the original size\n    upsampled_image = F.interpolate(zoomed_image, size=image.shape, mode='nearest')\n    \n    # Remove the extra dimensions and return the upsampled image as a PyTorch tensor\n    upsampled_image = upsampled_image.squeeze()\n    \n    return upsampled_image\n\ndef crop_3d_image(image, crop_size):\n    # Generate random starting coordinates within the valid range\n    start_x = torch.randint(0, image.shape[0] - crop_size[0] + 1, (1,))\n    start_y = torch.randint(0, image.shape[1] - crop_size[1] + 1, (1,))\n    start_z = torch.randint(0, image.shape[2] - crop_size[2] + 1, (1,))\n    \n    # Calculate the end coordinates based on the starting coordinates and crop size\n    end_x = start_x + crop_size[0]\n    end_y = start_y + crop_size[1]\n    end_z = start_z + crop_size[2]\n    \n    # Crop the image based on the computed coordinates\n    cropped_image = image[start_x:end_x, start_y:end_y, start_z:end_z]\n    \n    # Upsample the cropped image to the original size\n    upsampled_image = torch.nn.functional.interpolate(cropped_image.unsqueeze(0).unsqueeze(0), size=image.shape, mode='nearest')\n    \n    # Remove the extra dimensions and return the upsampled image as a PyTorch tensor\n    upsampled_image = upsampled_image.squeeze()\n    return upsampled_image\n\ndef crop_2d_image(image, crop_size):\n    # Generate random starting coordinates within the valid range\n    start_x = torch.randint(0, image.shape[0] - crop_size[0] + 1, (1,))\n    start_y = torch.randint(0, image.shape[1] - crop_size[1] + 1, (1,))\n    \n    # Calculate the end coordinates based on the starting coordinates and crop size\n    end_x = start_x + crop_size[0]\n    end_y = start_y + crop_size[1]\n    \n    # Crop the image based on the computed coordinates\n    cropped_image = image[start_x:end_x, start_y:end_y]\n    \n    # Upsample the cropped image to the original size\n    upsampled_image = torch.nn.functional.interpolate(cropped_image.unsqueeze(0).unsqueeze(0), size=image.shape,mode='nearest')\n    \n    # Remove the extra dimensions and return the upsampled image as a PyTorch tensor\n    upsampled_image = upsampled_image.squeeze()\n    \n    return upsampled_image\n    \n\nfor i, (data, target, idx) in enumerate(example_loader):\n    if torch.sum(target) > 10 and torch.sum(target) < 30:\n        print(data)\n        data = data.to(torch.float)\n        print(data.shape)\n        print(crop_3d_image(data[0],[12,22,22]))\n\n        break\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:15.669147Z","iopub.execute_input":"2023-06-08T21:40:15.669744Z","iopub.status.idle":"2023-06-08T21:40:15.807549Z","shell.execute_reply.started":"2023-06-08T21:40:15.669690Z","shell.execute_reply":"2023-06-08T21:40:15.806521Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"tensor([[[[0.3381, 0.3279, 0.3877,  ..., 0.3184, 0.3354, 0.3530],\n          [0.2739, 0.2803, 0.2922,  ..., 0.3340, 0.3518, 0.3623],\n          [0.3027, 0.3049, 0.2749,  ..., 0.3276, 0.3447, 0.3604],\n          ...,\n          [0.4141, 0.4165, 0.4214,  ..., 0.3733, 0.3816, 0.3967],\n          [0.3960, 0.3936, 0.3916,  ..., 0.3467, 0.3645, 0.3833],\n          [0.3176, 0.3047, 0.2932,  ..., 0.3916, 0.4148, 0.4348]],\n\n         [[0.3684, 0.3623, 0.3982,  ..., 0.3347, 0.3513, 0.3826],\n          [0.3020, 0.2908, 0.3276,  ..., 0.3425, 0.3623, 0.3745],\n          [0.2844, 0.2737, 0.2803,  ..., 0.3354, 0.3491, 0.3625],\n          ...,\n          [0.4026, 0.3997, 0.3984,  ..., 0.3823, 0.3872, 0.3992],\n          [0.3931, 0.3840, 0.3735,  ..., 0.3486, 0.3665, 0.3845],\n          [0.3418, 0.3250, 0.3079,  ..., 0.3899, 0.4104, 0.4277]],\n\n         [[0.4111, 0.4087, 0.4263,  ..., 0.3728, 0.3857, 0.4111],\n          [0.3442, 0.3330, 0.3689,  ..., 0.3704, 0.3857, 0.3979],\n          [0.2900, 0.2759, 0.3020,  ..., 0.3518, 0.3596, 0.3694],\n          ...,\n          [0.3730, 0.3662, 0.3567,  ..., 0.3665, 0.3750, 0.3823],\n          [0.3750, 0.3616, 0.3435,  ..., 0.3267, 0.3445, 0.3584],\n          [0.3574, 0.3491, 0.3318,  ..., 0.3652, 0.3777, 0.3894]],\n\n         ...,\n\n         [[0.3875, 0.3447, 0.3425,  ..., 0.6514, 0.6123, 0.5659],\n          [0.3569, 0.3442, 0.3508,  ..., 0.6646, 0.6650, 0.6313],\n          [0.3833, 0.3701, 0.3772,  ..., 0.5962, 0.6738, 0.6772],\n          ...,\n          [0.5107, 0.5068, 0.5029,  ..., 0.3933, 0.4268, 0.4612],\n          [0.5034, 0.5225, 0.5396,  ..., 0.4880, 0.5044, 0.5161],\n          [0.5293, 0.5542, 0.5874,  ..., 0.4932, 0.5000, 0.4973]],\n\n         [[0.3447, 0.3206, 0.3083,  ..., 0.5503, 0.5186, 0.4768],\n          [0.3191, 0.3025, 0.3032,  ..., 0.5586, 0.5693, 0.5488],\n          [0.3269, 0.3123, 0.3171,  ..., 0.4998, 0.5181, 0.6001],\n          ...,\n          [0.4773, 0.4771, 0.4729,  ..., 0.3618, 0.3879, 0.4136],\n          [0.4673, 0.4517, 0.4719,  ..., 0.4600, 0.4783, 0.4858],\n          [0.4927, 0.5005, 0.5298,  ..., 0.4841, 0.4915, 0.4863]],\n\n         [[0.3167, 0.2974, 0.2839,  ..., 0.4282, 0.4038, 0.3430],\n          [0.2729, 0.2725, 0.2708,  ..., 0.4199, 0.4287, 0.3411],\n          [0.2710, 0.2678, 0.2720,  ..., 0.3931, 0.4043, 0.4114],\n          ...,\n          [0.3936, 0.3926, 0.3474,  ..., 0.2954, 0.3225, 0.3357],\n          [0.3875, 0.3694, 0.3823,  ..., 0.3789, 0.3984, 0.3999],\n          [0.4080, 0.4106, 0.4333,  ..., 0.4209, 0.4255, 0.4197]]]],\n       dtype=torch.float16)\ntorch.Size([1, 13, 24, 24])\ntensor([[[0.3982, 0.3982, 0.3945,  ..., 0.3347, 0.3513, 0.3826],\n         [0.3982, 0.3982, 0.3945,  ..., 0.3347, 0.3513, 0.3826],\n         [0.3276, 0.3276, 0.3335,  ..., 0.3425, 0.3623, 0.3745],\n         ...,\n         [0.3845, 0.3845, 0.3943,  ..., 0.4714, 0.4670, 0.4692],\n         [0.4011, 0.4011, 0.4187,  ..., 0.4534, 0.4500, 0.4517],\n         [0.3984, 0.3984, 0.4104,  ..., 0.3823, 0.3872, 0.3992]],\n\n        [[0.3982, 0.3982, 0.3945,  ..., 0.3347, 0.3513, 0.3826],\n         [0.3982, 0.3982, 0.3945,  ..., 0.3347, 0.3513, 0.3826],\n         [0.3276, 0.3276, 0.3335,  ..., 0.3425, 0.3623, 0.3745],\n         ...,\n         [0.3845, 0.3845, 0.3943,  ..., 0.4714, 0.4670, 0.4692],\n         [0.4011, 0.4011, 0.4187,  ..., 0.4534, 0.4500, 0.4517],\n         [0.3984, 0.3984, 0.4104,  ..., 0.3823, 0.3872, 0.3992]],\n\n        [[0.4263, 0.4263, 0.4185,  ..., 0.3728, 0.3857, 0.4111],\n         [0.4263, 0.4263, 0.4185,  ..., 0.3728, 0.3857, 0.4111],\n         [0.3689, 0.3689, 0.3608,  ..., 0.3704, 0.3857, 0.3979],\n         ...,\n         [0.3413, 0.3413, 0.3479,  ..., 0.4465, 0.4414, 0.4446],\n         [0.3567, 0.3567, 0.3684,  ..., 0.4441, 0.4402, 0.4387],\n         [0.3567, 0.3567, 0.3643,  ..., 0.3665, 0.3750, 0.3823]],\n\n        ...,\n\n        [[0.3425, 0.3425, 0.3594,  ..., 0.6514, 0.6123, 0.5659],\n         [0.3425, 0.3425, 0.3594,  ..., 0.6514, 0.6123, 0.5659],\n         [0.3508, 0.3508, 0.3728,  ..., 0.6646, 0.6650, 0.6313],\n         ...,\n         [0.4998, 0.4998, 0.4836,  ..., 0.3115, 0.3167, 0.3335],\n         [0.5103, 0.5103, 0.5044,  ..., 0.3555, 0.3667, 0.3870],\n         [0.5029, 0.5029, 0.5078,  ..., 0.3933, 0.4268, 0.4612]],\n\n        [[0.3083, 0.3083, 0.3157,  ..., 0.5503, 0.5186, 0.4768],\n         [0.3083, 0.3083, 0.3157,  ..., 0.5503, 0.5186, 0.4768],\n         [0.3032, 0.3032, 0.3198,  ..., 0.5586, 0.5693, 0.5488],\n         ...,\n         [0.4329, 0.4329, 0.4155,  ..., 0.2625, 0.2634, 0.2778],\n         [0.4585, 0.4585, 0.4519,  ..., 0.3025, 0.3142, 0.3328],\n         [0.4729, 0.4729, 0.4395,  ..., 0.3618, 0.3879, 0.4136]],\n\n        [[0.2839, 0.2839, 0.2874,  ..., 0.4282, 0.4038, 0.3430],\n         [0.2839, 0.2839, 0.2874,  ..., 0.4282, 0.4038, 0.3430],\n         [0.2708, 0.2708, 0.2825,  ..., 0.4199, 0.4287, 0.3411],\n         ...,\n         [0.3381, 0.3381, 0.3223,  ..., 0.2134, 0.2097, 0.2192],\n         [0.3665, 0.3665, 0.2944,  ..., 0.2163, 0.2440, 0.2605],\n         [0.3474, 0.3474, 0.3479,  ..., 0.2954, 0.3225, 0.3357]]])\n","output_type":"stream"}]},{"cell_type":"code","source":"data2 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/2\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,boundarysize = boundarysize+500,calibrator=calibrator1)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:34:43.684616Z","iopub.execute_input":"2023-06-08T21:34:43.685112Z","iopub.status.idle":"2023-06-08T21:36:38.492982Z","shell.execute_reply.started":"2023-06-08T21:34:43.685078Z","shell.execute_reply":"2023-06-08T21:36:38.491395Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (140973980 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\nmerged_dataset = ConcatDataset([data1, data2])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:36:38.495320Z","iopub.execute_input":"2023-06-08T21:36:38.495807Z","iopub.status.idle":"2023-06-08T21:36:38.503040Z","shell.execute_reply.started":"2023-06-08T21:36:38.495766Z","shell.execute_reply":"2023-06-08T21:36:38.501600Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport random\n\nclass RandomNoise(object):\n    def __init__(self, mean=0.0, std=1.0):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        noise = torch.randn_like(tensor) * self.std + self.mean\n        return tensor + torch.abs(noise)\n    \ndef rotate_image(data,target):\n    degrees = np.random.choice([90, 180, 270])\n    rotated_data = np.rot90(data, k=degrees // 90, axes= (-2, -1)).copy()\n    rotated_target = np.rot90(target, k=degrees // 90, axes=(-2, -1)).copy()\n    return torch.tensor(rotated_data),torch.tensor(rotated_target)\n\ndef random_shift(data,target, max_shift=4):\n    x_shift = np.random.randint(-max_shift, max_shift)\n    y_shift = np.random.randint(-max_shift, max_shift)\n    shifted_data = torch.roll(data, shifts=(x_shift, y_shift), dims=(-2, -1))\n    shifted_target = torch.roll(target, shifts=(x_shift, y_shift), dims=(-2, -1))\n    \n    return shifted_data,shifted_target\n\nclass CNN3D(nn.Module):\n    def __init__(self):\n        super(CNN3D, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(576  , 64)\n        \n        self.saved_loss = []\n        self.saved_epoch = []\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = self.conv3(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = x.view(x.size(0), -1)  \n        x = self.fc(x)\n        return x\n    \n    def augment(self, data, target):\n        batch_size = data.size(0)\n        rands = torch.rand(batch_size)\n\n        for i in range(batch_size):\n            if rands[i] < 0.1:\n                data[i] = crop_3d_image(data[i],[13,22,22])\n                target[i] = crop_2d_image(target[i],[7,7])\n            elif rands[i] < 0.25:\n                zoomf = random.uniform(0.7, 0.95)\n                data[i] = zoom_3d_image(data[i],zoomf)\n                target[i] = zoom_2d_image(target[i],zoomf)\n            elif rands[i] < 0.35:\n                data[i], target[i] = rotate_image(data[i],target[i])\n\n                \n        return data, target\n\n\n    def fit(self, train_data,optimizer, criterion, num_epochs,verbose=False): \n        train_loader = DataLoader(train_data, batch_size = 512, shuffle=True)\n        for epoch in range(num_epochs):\n            loss_list = []\n            f1_list = []\n            for batch_idx, (data, target, idxs) in tqdm(enumerate(train_loader)):\n                data = data.to(torch.float)\n                data, target = self.augment(data,target)\n                inputs = data.view(data.shape[0],1, *data.shape[1:])\n                inputs = inputs.to(device)\n                output = (self(inputs))\n                target = target.to(device)\n                target = target.to(torch.float)\n                optimizer.zero_grad()\n                loss = criterion(output, target.view(target.shape[0], -1))\n                l2_lambda = 0.0005 \n                l2_regularization = torch.tensor(0.)\n                l2_regularization =  l2_regularization.to(device)\n                for param in model.parameters():\n                    l2_regularization += torch.norm(param, p=2)\n                    loss += l2_lambda * l2_regularization\n                \n                loss_list.append(loss.item())\n                loss.backward()\n                optimizer.step()\n                if batch_idx % 100 == 0:\n                    print(np.mean(loss_list))\n                \n            if verbose == True:\n                epoch_loss = np.mean(loss_list)\n                self.save_loss(epoch_loss,epoch)\n                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n            \n                \n    def save_loss(self,epoch_loss,epoch):\n        self.saved_loss.append(epoch_loss)\n        if len(self.saved_epoch) != 0:\n            if self.saved_epoch[-1] > epoch:\n                self.saved_epoch.append(self.saved_epoch[-1]+1)\n            else:\n                 self.saved_epoch.append(epoch)\n        else:\n            self.saved_epoch.append(epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:20.916921Z","iopub.execute_input":"2023-06-08T21:40:20.917352Z","iopub.status.idle":"2023-06-08T21:40:20.956781Z","shell.execute_reply.started":"2023-06-08T21:40:20.917319Z","shell.execute_reply":"2023-06-08T21:40:20.955446Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class HybridLoss(nn.Module):\n    def __init__(self, weight_dice=0.5, weight_bce=0.5):\n        super(HybridLoss, self).__init__()\n        self.weight_dice = weight_dice\n        self.weight_bce = weight_bce\n        self.bce_loss = nn.BCEWithLogitsLoss()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, predicted, target):\n        predicted_sigmoid = self.sigmoid(predicted)\n        dice_loss = self.dice_loss(predicted_sigmoid, target)\n        bce_loss = self.bce_loss(predicted, target)\n        hybrid_loss = (self.weight_dice * dice_loss) + (self.weight_bce * bce_loss)\n        return hybrid_loss\n\n    def dice_loss(self, predicted, target):\n        H = 8\n        W = 8\n        predicted = predicted.view(-1, H, W)  \n        target = target.view(-1, H, W) \n        smooth = 1e-5\n        intersection = torch.sum(predicted * target)\n        union = torch.sum(predicted) + torch.sum(target)\n        dice_coefficient = (2 * intersection + smooth) / (union + smooth)\n        dice_loss = 1 - dice_coefficient\n        return dice_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:21.617389Z","iopub.execute_input":"2023-06-08T21:40:21.617822Z","iopub.status.idle":"2023-06-08T21:40:21.629444Z","shell.execute_reply.started":"2023-06-08T21:40:21.617791Z","shell.execute_reply":"2023-06-08T21:40:21.627831Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model = CNN3D()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001,lr=0.005)\ncriterion = HybridLoss()\nmodel.load_state_dict(torch.load(\"/kaggle/input/3dweights/new3d-2.pth\", map_location=torch.device('cpu')))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:22.666419Z","iopub.execute_input":"2023-06-08T21:40:22.667992Z","iopub.status.idle":"2023-06-08T21:40:22.709742Z","shell.execute_reply.started":"2023-06-08T21:40:22.667930Z","shell.execute_reply":"2023-06-08T21:40:22.708481Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"CNN3D(\n  (conv1): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (fc): Linear(in_features=576, out_features=64, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001,lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:24.522307Z","iopub.execute_input":"2023-06-08T21:40:24.523521Z","iopub.status.idle":"2023-06-08T21:40:24.530619Z","shell.execute_reply.started":"2023-06-08T21:40:24.523450Z","shell.execute_reply":"2023-06-08T21:40:24.529344Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\nmodel.fit(merged_dataset,optimizer,criterion,num_epochs,verbose = True)\n#torch.save(model.state_dict(), 'new_3dweights.pth')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:40:27.700503Z","iopub.execute_input":"2023-06-08T21:40:27.700974Z","iopub.status.idle":"2023-06-08T22:19:30.199090Z","shell.execute_reply.started":"2023-06-08T21:40:27.700929Z","shell.execute_reply":"2023-06-08T22:19:30.196887Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"1it [00:03,  3.88s/it]","output_type":"stream"},{"name":"stdout","text":"0.8888023495674133\n","output_type":"stream"},{"name":"stderr","text":"101it [04:45,  2.89s/it]","output_type":"stream"},{"name":"stdout","text":"0.841376024307591\n","output_type":"stream"},{"name":"stderr","text":"201it [09:55,  2.82s/it]","output_type":"stream"},{"name":"stdout","text":"0.8252195822658823\n","output_type":"stream"},{"name":"stderr","text":"301it [14:40,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"0.8109987665252432\n","output_type":"stream"},{"name":"stderr","text":"401it [19:52,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"0.7998140814001126\n","output_type":"stream"},{"name":"stderr","text":"501it [24:38,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"0.7890538202074473\n","output_type":"stream"},{"name":"stderr","text":"601it [29:26,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"0.7798403023483352\n","output_type":"stream"},{"name":"stderr","text":"701it [34:12,  2.81s/it]","output_type":"stream"},{"name":"stdout","text":"0.7713779329233265\n","output_type":"stream"},{"name":"stderr","text":"801it [39:00,  2.91s/it]","output_type":"stream"},{"name":"stdout","text":"0.7637837669822607\n","output_type":"stream"},{"name":"stderr","text":"801it [39:01,  2.92s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), 'new_3dweights.pth')\u001b[39;00m\n","Cell \u001b[0;32mIn[43], line 85\u001b[0m, in \u001b[0;36mCNN3D.fit\u001b[0;34m(self, train_data, optimizer, criterion, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m inputs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     84\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 85\u001b[0m output \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     86\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[43], line 45\u001b[0m, in \u001b[0;36mCNN3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:868\u001b[0m, in \u001b[0;36m_max_pool3d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'modelnewaugments.pth')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:19:35.467471Z","iopub.execute_input":"2023-06-08T22:19:35.469198Z","iopub.status.idle":"2023-06-08T22:19:35.478278Z","shell.execute_reply.started":"2023-06-08T22:19:35.469140Z","shell.execute_reply":"2023-06-08T22:19:35.477162Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"data3 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/3\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,calibrator=calibrator2)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:19:41.587791Z","iopub.execute_input":"2023-06-08T22:19:41.588269Z","iopub.status.idle":"2023-06-08T22:20:09.342108Z","shell.execute_reply.started":"2023-06-08T22:19:41.588235Z","shell.execute_reply":"2023-06-08T22:20:09.340936Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"score = []\npreds = []\ntargs = []\nval_loader =DataLoader(data3, batch_size = 512)\nfor data, target,idxs in tqdm((val_loader)):\n    data = data.to(torch.float)\n    output = model(data.view(data.shape[0],1,*data.shape[1:]).to(device))\n    target = target.to(device).view(target.shape[0],64)\n    target = target.to(output.dtype)\n    score.append(nn.BCEWithLogitsLoss(pos_weight=torch.tensor(3))(output, target).item())\n    preds.append((nn.Sigmoid()(output)).detach().cpu().numpy())\n    targs.append(target.detach().cpu().numpy())\n\nprint(np.mean(score))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:20:09.344271Z","iopub.execute_input":"2023-06-08T22:20:09.344732Z","iopub.status.idle":"2023-06-08T22:40:04.837153Z","shell.execute_reply.started":"2023-06-08T22:20:09.344700Z","shell.execute_reply":"2023-06-08T22:40:04.835969Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"100%|██████████| 769/769 [19:55<00:00,  1.55s/it]","output_type":"stream"},{"name":"stdout","text":"0.758097517707413\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class Fscore:\n    def __init__(self):\n        self.epsilon = 1e-7\n        self.tp = 0\n        self.fp = 0\n        self.fn = 0\n    \n    def accumulate(self, y_true, y_pred):\n        # Flatten the input arrays to 1D\n        y_true = np.ravel(y_true)\n        y_pred = np.ravel(y_pred)\n        \n        # Update true positives, false positives, and false negatives\n        self.tp += np.sum((y_true == 1) & (y_pred == 1))\n        self.fp += np.sum((y_true == 0) & (y_pred == 1))\n        self.fn += np.sum((y_true == 1) & (y_pred == 0))\n    \n    def get_score(self, beta=1):\n       # Calculate precision, recall, and F-score\n        precision = self.tp / (self.tp + self.fp + self.epsilon)\n        recall = self.tp / (self.tp + self.fn + self.epsilon)\n        f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + self.epsilon)\n    \n        return f_score","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:40:04.839286Z","iopub.execute_input":"2023-06-08T22:40:04.839766Z","iopub.status.idle":"2023-06-08T22:40:04.850718Z","shell.execute_reply.started":"2023-06-08T22:40:04.839724Z","shell.execute_reply":"2023-06-08T22:40:04.849474Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\n\ndef round_elements(arr, threshold):\n    rounded_arr = []\n    for elem in tqdm(arr):\n        rounded_elem = np.zeros_like(elem)\n        for i in range(elem.shape[0]):\n            for j in range(elem.shape[1]):\n                if elem[i, j] < threshold:\n                    rounded_elem[i, j] = math.floor(elem[i, j])  # Round down\n                else:\n                    rounded_elem[i, j] = math.ceil(elem[i, j])   # Round up\n        rounded_arr.append(rounded_elem)\n    return rounded_arr","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:40:37.246538Z","iopub.execute_input":"2023-06-08T22:40:37.246969Z","iopub.status.idle":"2023-06-08T22:40:37.255357Z","shell.execute_reply.started":"2023-06-08T22:40:37.246939Z","shell.execute_reply":"2023-06-08T22:40:37.254385Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"threshold = 0.6\n\nrounded_numbers = round_elements(preds,threshold)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:43:40.506520Z","iopub.execute_input":"2023-06-08T22:43:40.507722Z","iopub.status.idle":"2023-06-08T22:45:32.125341Z","shell.execute_reply.started":"2023-06-08T22:43:40.507669Z","shell.execute_reply":"2023-06-08T22:45:32.124103Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"100%|██████████| 769/769 [01:51<00:00,  6.89it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"score = Fscore()\nfor p,t in zip(rounded_numbers,targs):\n     score.accumulate(t,p)\n        \nprint(score.get_score(beta=0.5))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T22:45:32.127769Z","iopub.execute_input":"2023-06-08T22:45:32.128110Z","iopub.status.idle":"2023-06-08T22:45:32.296033Z","shell.execute_reply.started":"2023-06-08T22:45:32.128079Z","shell.execute_reply":"2023-06-08T22:45:32.294790Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"0.270818906390613\n","output_type":"stream"}]},{"cell_type":"code","source":"(0.76,27.4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PictureBuilder:\n    def __init__(self,snippet_width=16,snippet_height=16):\n        self.full_width = 6452\n        self.full_height = 8298\n    \n        self.snippet_width = snippet_width\n        self.snippet_height = snippet_height\n\n        self.full_picture = np.zeros((self.full_height, self.full_width))\n        \n\n    def __call__(self,indx_matrix,snippet_matrix):\n        height = indx_matrix[0]\n        width = indx_matrix[1]\n        temp = insert_snippet(self.full_picture,height,width,self.snippet_height\\\n                       ,self.snippet_width,snippet_matrix)\n        self.full_picture = temp\n        \n    \n    def _get(self):\n        return self.full_picture\n\n@numba.njit()\ndef insert_snippet(full_picture,height,width,sheight,swidth,smatrix):\n    for i in range(sheight):\n        for j in range(swidth):\n            full_picture[height[i],width[j]] = smatrix[i,j]\n    return full_picture","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"picture = PictureBuilder(snippet_width=8,snippet_height=8)\npics_loader = DataLoader(data1, batch_size=1)\nfor batch_idx, (x_data, target, indx_arr) in tqdm(enumerate(pics_loader)):\n    x_data = x_data.to(torch.float)\n    x_data = x_data.view(1,*x_data.shape)\n    x_data = torch.tensor((x_data).to(device))\n    pred =  torch.sigmoid(model(x_data))\n    pred_cpu = pred.detach().cpu().numpy()\n    picture(indx_arr[0, :].T.numpy(), pred_cpu.reshape(8, 8))\n    if batch_idx > 400000:\n        break\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.round(picture._get()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"picture2 = PictureBuilder(snippet_width=8,snippet_height=8)\npics_loader = DataLoader(data2, batch_size=1)\nfor batch_idx, (x_data, target, indx_arr) in tqdm(enumerate(pics_loader)):\n    x_data = x_data.to(torch.float)\n    x_data = x_data.view(1,*x_data.shape)\n    x_data = torch.tensor((x_data).to(device))\n    pred =  torch.sigmoid(model(x_data))\n    pred_cpu = pred.detach().cpu().numpy()\n    picture2(indx_arr[0, :].T.numpy(), pred_cpu.reshape(8, 8))\n    if batch_idx > 400000:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.round(picture2._get()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport imageio\n\n# Define threshold range\nthreshold_range = np.arange(0.05, 0.7, 0.02)\n\n# Create empty image sequence\nimages = []\n\n# Iterate over each threshold\nfor threshold in threshold_range:\n\n    # Apply rounding based on the threshold\n    rounded_pixels = np.round(picture._get() < threshold)\n\n    # Create a grayscale image from the rounded pixels\n    image = Image.fromarray((rounded_pixels*255).astype(np.uint8), mode='L')\n\n    # Append image to the sequence\n    images.append(image)\n\n# Save the sequence as a GIF\nimageio.mimsave('grid_rounding.gif', images, duration=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}