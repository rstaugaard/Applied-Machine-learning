{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport glob\nimport PIL.Image as Image\nimport torch.utils.data as data\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im1_mean = [0.4042127201928427, 0.412576527722026, 0.42259604676925305, 0.43350413711051056, 0.443855690032959, 0.45140969585826785, 0.45334100553503703, 0.4468871853247354, 0.43043786530179756, 0.404718756014092, 0.37331931684981945, 0.3418279141576013,0.31582148226006085]\nim2_mean = [0.41915108148831637, 0.42160734423110063, 0.423962258067611, 0.42601487354034223, 0.4275087523492551, 0.4281458628877129, 0.4276228290175893, 0.4257433505811504, 0.4224049007987228, 0.41766727063885856, 0.411706916417739, 0.4048067182780746,0.39733730345961626]\nim3_mean = [0.44033469101140527, 0.4503952708250446, 0.46193364562084055, 0.4738397130390685, 0.48420565326379844, 0.49040465635549285, 0.48956380687421636, 0.47946231783341775, 0.45967343462004484, 0.4323601088249301, 0.4019184607310354, 0.37350198569667487,0.3512268327779888]\n\nim1_std = [0.14732023241999434, 0.147809241675677, 0.14804508827283613, 0.1477574222750175, 0.1470682801149269, 0.146539035212084, 0.14675348492637386, 0.14775991989824996, 0.14848621292810835, 0.1465088569090591, 0.13925408874284476, 0.12590440973744257,0.10823487264730804]\nim2_std = [0.12246606074712985, 0.12329658876446659, 0.1242021620079717, 0.12510900974078862, 0.12595538841389356, 0.12670439991891755, 0.12730158828399146, 0.12765988385551708, 0.12760264169433708, 0.1269505535408886, 0.12552824561659184, 0.12318955457930125,0.11984089703152391]\nim3_std = [0.11690682253979083, 0.11736897240085645, 0.11786639327518504, 0.11859482672002022, 0.11987161521520828, 0.1221401983344635, 0.12578272075012656, 0.13025746011922526, 0.13357738663561808, 0.13312490121042017, 0.12725185198165137, 0.11619303606414197,0.10181700640611481]\n\ncalibrator1 = {}\ncalibrator2 = {}\n\nfor layer_idx in range(13):\n    def calibration1(layer_data):\n        mask = (layer_data >= 0.1) & (layer_data <= 0.9) \n        scaled_x = np.copy(layer_data)  \n        scaled_x[mask] = ((layer_data[mask] - im2_mean[layer_idx])/im2_std[layer_idx]) * im1_std[layer_idx] + im1_mean[layer_idx] \n        return scaled_x\n    \n    def calibration2(layer_data):\n        mask = (layer_data >= 0.1) & (layer_data <= 0.9) \n        scaled_x = np.copy(layer_data)  \n        scaled_x[mask] = ((layer_data[mask] - im3_mean[layer_idx])/im3_std[layer_idx]) * im1_std[layer_idx] + im1_mean[layer_idx] \n        return scaled_x    \n\n    calibrator1[layer_idx] = calibration1\n    calibrator2[layer_idx] = calibration2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numba\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom typing import Tuple, List\nfrom torchvision.transforms import transforms\nimport cv2\n\nclass VesuviusTrainData(Dataset):\n    _relative_sv_dir = \"surface_volume\"  # relative to the directory of train data\n\n    def __init__(self,\n                 dir_path: str = \"train/1\",\n                 z_start: int = 0,\n                 z_end: int = 64,\n                 nucleus_shape: Tuple[int, int] = (16, 16),\n                 hull_size: Tuple[int, int] = (64, 64),\n                 compress_depth =  None,\n                 give_indx = False,\n                 calibrator = None,\n                 boundarysize = None\n                 ):\n        self.dir_path = dir_path\n\n        self.z_start = z_start\n        self.z_end = z_end\n\n        self.nucleus_shape = nucleus_shape\n        self.hull_size = hull_size\n        \n        self.compress_depth = compress_depth\n        self.give_indx = give_indx\n        self.calibrator = calibrator\n        self.boundarysize = boundarysize\n        \n        \n        self._setup()\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, index: int):\n        i, j = self._get_pixel_from_index(self.indices[index])\n        nucleus_height, nucleus_width = self.nucleus_shape\n\n        if self.give_indx == False:\n            if self.calibrator != None:\n                return (torch.from_numpy(\n                    self.calibrate(self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ])),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]))\n            else:\n                return (torch.from_numpy(\n                    self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ]),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height])) \n        else:\n            if self.calibrator != None:\n                return (torch.from_numpy(\n                    self.calibrate(self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ])),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]),\n                   np.stack(([i+k for k in range(nucleus_width)],[\n                    j+k for k in range(nucleus_height)]),axis=1))\n            else:\n                return (torch.from_numpy(\n                    self.images[\n                    :,\n                    i - self._left_hull + self._left_pad_image:\n                    i + nucleus_width + self._right_hull + self._left_pad_image,\n                    j - self._top_hull + self._top_pad_image:\n                    j + nucleus_height + self._bottom_hull + self._top_pad_image\n                     ]),\n                    torch.from_numpy(\n                    self.inklabels[i: i + nucleus_width,\n                    j: j + nucleus_height]),\n                   np.stack(([i+k for k in range(nucleus_width)],[\n                    j+k for k in range(nucleus_height)]),axis=1)) \n            \n    def calibrate(self,arr):\n        calibrated_arr = np.zeros_like(arr)\n\n        for i in range(arr.shape[0]):\n            calibration_func = self.calibrator[i]\n            calibrated_arr[i, :, :] = calibration_func(arr[i, :, :])\n        return calibrated_arr\n\n\n    def _load_images(self, z_start=None,z_end=None):\n        if z_end is None:\n            z_end = self.z_end \n        if z_start is None:\n            z_start = self.z_start\n            \n        num_images = z_end - z_start\n        self.images = np.empty(\n            (\n                num_images,\n                self.mask.shape[0] + self._left_pad_image + self._right_pad_image,\n                self.mask.shape[1] + self._top_pad_image + self._bottom_pad_image\n            ), \n            dtype=np.float16\n        )\n        for index, i in enumerate(range(z_start, z_end)):\n            # noinspection PyTypeChecker\n            image = np.array(Image.open(f\"{self.dir_path}\"\n                                        f\"/{self._relative_sv_dir}\"\n                                        f\"/{i:02d}.tif\"),\n                             dtype=np.float32) / 65535.0\n            \n            image = image.astype(np.float16)\n            \n            image = np.pad(image, ((self._left_pad_mask + self._left_pad_image,\n                                    self._right_pad_mask + self._right_pad_image),\n                                   (self._top_pad_mask + self._top_pad_image,\n                                    self._bottom_pad_mask + self._bottom_pad_image)),\n                           'constant', constant_values=0)\n            self.images[index, :, :] = image\n            \n    def _load_scale(self):\n        current_stack_layer = self.z_start\n        images = np.empty(\n            (\n                len(self.compress_depth),\n                self.mask.shape[0] + self._left_pad_image + self._right_pad_image,\n                self.mask.shape[1] + self._top_pad_image + self._bottom_pad_image\n            ), \n            dtype=np.float16\n        )\n        for index,layer in enumerate(self.compress_depth):\n            self._load_images(current_stack_layer, current_stack_layer + layer)\n            current_stack_layer += layer\n            layer_stack = self.images\n            images[index,:,:] = np.mean(layer_stack,axis = 0)\n            \n        self.images = images\n        \n        \n    def crop_boundary(self,mask, boundary_size):\n        mask = mask.astype(np.uint8)\n        mask_binary = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY)[1]\n\n        contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        boundary_mask = np.ones_like(mask, dtype=np.uint8)\n\n        cv2.drawContours(boundary_mask, contours, -1, (0), 100)\n\n        kernel_size = int(boundary_size)  # Adjust the kernel size based on your needs\n        kernel = np.ones((kernel_size, kernel_size), dtype=np.uint8)\n        eroded_boundary_mask = cv2.erode(boundary_mask, kernel, iterations=1)\n\n        cropped_mask = mask * eroded_boundary_mask[:, :]\n\n        return cropped_mask\n    \n    # Internal utility methods\n    def _setup(self):\n        # noinspection PyTypeChecker\n        self.mask = np.array(Image.open(f\"{self.dir_path}/mask.png\"))\n        if (self.boundarysize) != None:\n            self.mask = self.crop_boundary(self.mask,self.boundarysize)\n        # noinspection PyTypeChecker\n        self.inklabels = np.array(Image.open(f\"{self.dir_path}/inklabels.png\"))\n        self._granulate_mask()\n        self._fill_new_mask()\n        if self.compress_depth is None:\n            self._load_images()\n        else:\n            self._load_scale()\n        self._get_leftover_hull()\n\n    def _granulate_mask(self):\n        self._get_paddings()\n        self.mask = np.pad(self.mask,\n                           ((self._left_pad_mask, self._right_pad_mask),\n                            (self._top_pad_mask, self._bottom_pad_mask)),\n                           'constant', constant_values=0)\n        self.indices = _find_labeled_nucleus(self.mask, self.nucleus_shape)\n        self.inklabels = np.pad(self.inklabels, \n                                ((self._left_pad_mask, self._right_pad_mask),\n                                 (self._top_pad_mask, self._bottom_pad_mask)),\n                                'constant', \n                                constant_values=0)\n\n    def _get_paddings(self):\n        width, height = self.mask.shape\n        nucleus_width, nucleus_height = self.nucleus_shape\n        hull_width, hull_height = self.hull_size\n        self._left_pad_mask = (width % nucleus_width) // 2\n        self._right_pad_mask = width % nucleus_width - self._left_pad_mask\n        self._top_pad_mask = (height % nucleus_height) // 2\n        self._bottom_pad_mask = height % nucleus_height - self._top_pad_mask\n\n        self._left_pad_image = (hull_width - nucleus_width) // 2\n        self._right_pad_image = hull_width - nucleus_width - self._left_pad_image\n        self._top_pad_image = (hull_height - nucleus_height) // 2\n        self._bottom_pad_image = hull_height - nucleus_height - self._top_pad_image\n\n    def _fill_new_mask(self):\n        nucleus_width, nucleus_height = self.nucleus_shape\n        self.mask = np.zeros_like(self.mask)\n        for index in self.indices:\n            i, j = self._get_pixel_from_index(index)\n            self.mask[\n                i * nucleus_width: (i + 1) * nucleus_width,\n                j * nucleus_height: (j + 1) * nucleus_height\n            ] = 1\n\n    def _get_pixel_from_index(self, index: int):\n        mask_width, mask_height = self.mask.shape\n        nucleus_width, nucleus_height = self.nucleus_shape\n        height = mask_height // nucleus_height\n        return index // height * nucleus_height, index % height * nucleus_width\n\n    def _get_leftover_hull(self):\n        nucleus_width, nucleus_height = self.nucleus_shape\n        hull_width, hull_height = self.hull_size\n        self._left_hull = (hull_width - nucleus_width) // 2\n        self._right_hull = hull_width - nucleus_width - self._left_hull\n        self._top_hull = (hull_height - nucleus_height) // 2\n        self._bottom_hull = hull_height - nucleus_height - self._top_hull\n\n\n@numba.njit()\ndef _find_labeled_nucleus(mask, nuclues_shape):\n    width, height = mask.shape\n    nucleus_width, nucleus_height = nuclues_shape\n\n    indices = - np.ones((width // nucleus_width) * (height // nucleus_height), dtype=np.int32)\n    current_index = 0\n    for i in range(width // nucleus_width):\n        for j in range(height // nucleus_height):\n            if np.sum(\n                    mask[i * nucleus_width: (i + 1) * nucleus_width,\n                         j * nucleus_height: (j + 1) * nucleus_height]\n                      ) > 0:\n                indices[current_index] = i * (height // nucleus_height) + j\n                current_index += 1\n    return indices[indices != -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nucleus_shape = (8,8)\nhull_size = (24,24)\nz_start = 22\nz_end = 35\nboundarysize = 700","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/1\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,boundarysize = boundarysize+200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/2\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,boundarysize = boundarysize+500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom torch.utils.data import DataLoader\n\n\ndef bm3d_denoise(patch, block_size=10, search_window_size=16, sigma=20):\n    # Step 2: Block matching and collaborative filtering\n    denoised_patch = np.zeros_like(patch)\n    height, width = patch.shape[:2]\n    blocks_per_row = width // block_size\n    blocks_per_col = height // block_size\n    \n    for j in range(blocks_per_col):\n        for i in range(blocks_per_row):\n            # Extract the current block\n            block = patch[j*block_size:(j+1)*block_size, i*block_size:(i+1)*block_size]\n            \n            # Define the search window boundaries\n            window_start_x = max(0, i*block_size - search_window_size)\n            window_end_x = min((i+1)*block_size + search_window_size, width)\n            window_start_y = max(0, j*block_size - search_window_size)\n            window_end_y = min((j+1)*block_size + search_window_size, height)\n            \n            # Extract the search window\n            search_window = patch[window_start_y:window_end_y, window_start_x:window_end_x]\n            \n            # Calculate Euclidean distances between the block and search window blocks\n            distances = np.sum(np.square(block - search_window.reshape(-1, block_size, block_size)), axis=(1, 2))\n            \n            # Sort the distances and select similar blocks\n            sorted_indices = np.argsort(distances)\n            similar_blocks_indices = sorted_indices[:10]  # Choose the top 10 similar blocks\n            \n            # Collaborative filtering\n            similar_blocks = search_window[similar_blocks_indices]\n            weights = np.exp(-distances[similar_blocks_indices] / (2 * sigma**2))\n            weighted_sum = np.sum(similar_blocks * weights[:, np.newaxis, np.newaxis], axis=0)\n            sum_of_weights = np.sum(weights)\n            filtered_block = weighted_sum / sum_of_weights\n            \n            # Update the denoised patch\n            denoised_patch[j*block_size:(j+1)*block_size, i*block_size:(i+1)*block_size] = filtered_block\n    \n    return denoised_patch\n\ntrain_loader = DataLoader(data1, batch_size = 1, shuffle=True)\n\noutput_patches = []\noutput_targs = []\n\nfor input_patch, targ,idx in tqdm(train_loader):\n    denoised_patch = bm3d_denoise(input_patch)\n    output_patches.append(denoised_patch)\n    output_targs.append(targ)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(data2, batch_size = 1, shuffle=True)\noutput_patches2 = []\noutput_targs2 = []\n\nfor input_patch, targ,idx in tqdm(train_loader):\n    denoised_patch = bm3d_denoise(input_patch)\n    output_patches2.append(denoised_patch)\n    output_targs2.append(targ)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\ntar = torch.stack(output_targs)\ntar2 = torch.stack(output_targs2)\ndataset1 = TensorDataset(torch.tensor(np.array(output_patches)),torch.tensor(tar))\ndataset2 = TensorDataset(torch.tensor(np.array(output_patches2)),torch.tensor(tar2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\nmerged_dataset = ConcatDataset([dataset1, dataset2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport random\n\nclass RandomNoise(object):\n    def __init__(self, mean=0.0, std=1.0):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        noise = torch.randn_like(tensor) * self.std + self.mean\n        return tensor + torch.abs(noise)\n    \ndef rotate_image(data,target):\n    degrees = np.random.choice([90, 180, 270])\n    rotated_data = np.rot90(data, k=degrees // 90, axes= (-2, -1)).copy()\n    rotated_target = np.rot90(target, k=degrees // 90, axes=(-2, -1)).copy()\n    return torch.tensor(rotated_data),torch.tensor(rotated_target)\n\ndef random_shift(data,target, max_shift=2):\n    x_shift = np.random.randint(-max_shift, max_shift)\n    y_shift = np.random.randint(-max_shift, max_shift)\n    z_shift = np.random.randint(-max_shift, max_shift)\n    shifted_data = torch.roll(data, shifts=(x_shift, y_shift,z_shift), dims=(-3,-2, -1))\n    shifted_target = torch.roll(target, shifts=(x_shift, y_shift), dims=(-2, -1))\n    \n    return shifted_data,shifted_target\n\nclass CNN3D(nn.Module):\n    def __init__(self):\n        super(CNN3D, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=(3,3,3), padding=1)\n        self.bn1 = nn.BatchNorm3d(16)\n        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(3,2,2), padding=1)\n        self.bn2 = nn.BatchNorm3d(32)\n        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(3,1,1), padding=1)\n        self.bn3 = nn.BatchNorm3d(64)\n        self.fc = nn.Linear(1024  , 64)\n        \n        self.saved_loss = []\n        self.saved_epoch = []\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool3d(x, kernel_size=2)\n        x = x.view(x.size(0), -1)  \n        x = self.fc(x)\n        return x\n    \n    def augment(self, data, target):\n        batch_size = data.size(0)\n        rands = torch.rand(batch_size)\n        flip1 = transforms.RandomApply([transforms.RandomHorizontalFlip()], p=1)\n        flip2 = transforms.RandomApply([transforms.RandomVerticalFlip()], p=1)\n        noise = transforms.RandomApply([RandomNoise(mean=0, std=0.005)], p=1)\n        data_list = []\n        tar_list = []\n\n        for i in range(batch_size):\n            if torch.sum(target[i]) == 0:\n                random_number = random.random() \n                if random_number > 0.4:\n                    continue\n            \n            if rands[i] < 0.1:\n                dat, tar= rotate_image(data[i],target[i])\n                data_list.append(dat)\n                tar_list.append(tar)\n            elif rands[i] < 0.3:\n                dat,tar = random_shift(data[i],target[i]) \n                data_list.append(dat)\n                tar_list.append(tar)\n            else:\n                data_list.append(data[i])\n                tar_list.append(target[i])\n                \n        return  torch.stack(data_list), torch.stack(tar_list)\n\n\n    def fit(self, train_data,optimizer, criterion, num_epochs,verbose=False): \n        train_loader = DataLoader(train_data, batch_size = 512, shuffle=True)\n        for epoch in range(num_epochs):\n            loss_list = []\n            f1_list = []\n            for batch_idx, (data, target, idxs) in tqdm(enumerate(train_loader)):\n                data = data.to(torch.float)\n                data, target = self.augment(data,target)\n                inputs = data.view(data.shape[0],1, *data.shape[1:])\n                inputs = inputs.to(device)\n                output = (self(inputs))\n                target = target.to(device)\n                target = target.to(torch.float)\n                optimizer.zero_grad()\n                loss = criterion(output, target.view(target.shape[0], -1))\n                loss_list.append(loss.item())\n                loss.backward()\n                optimizer.step()\n                if batch_idx % 100 == 0:\n                    print(np.mean(loss_list))\n                \n            if verbose == True:\n                epoch_loss = np.mean(loss_list)\n                self.save_loss(epoch_loss,epoch)\n                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n            \n                \n    def save_loss(self,epoch_loss,epoch):\n        self.saved_loss.append(epoch_loss)\n        if len(self.saved_epoch) != 0:\n            if self.saved_epoch[-1] > epoch:\n                self.saved_epoch.append(self.saved_epoch[-1]+1)\n            else:\n                 self.saved_epoch.append(epoch)\n        else:\n            self.saved_epoch.append(epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN3D()\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n#model.load_state_dict(torch.load(\"/kaggle/input/3dweights/new3d-2.pth\", map_location=torch.device('cpu')))\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 3\nmodel.fit(merged_dataset,optimizer,criterion,num_epochs,verbose = True)\n#torch.save(model.state_dict(), 'new_3dweights.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'again.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = VesuviusTrainData(\n    dir_path=\"/kaggle/input/vesuvius-challenge-ink-detection/train/3\"\\\n    ,nucleus_shape = nucleus_shape,hull_size=hull_size, z_start=z_start\n    , z_end=z_end,give_indx=True,calibrator=calibrator2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(data3, batch_size = 1, shuffle=True)\n\noutput_patches3 = []\noutput_targs3 = []\n\nfor input_patch, targ,idx in tqdm(train_loader):\n    denoised_patch = bm3d_denoise(input_patch)\n    output_patches3.append(denoised_patch)\n    output_targs3.append(targ)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tar3 = torch.stack(output_targs3)\n\ndataset3 = TensorDataset(torch.tensor(np.array(output_patches3)),torch.tensor(tar3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\npreds = []\ntargs = []\nval_loader =DataLoader(dataset3, batch_size = 512,shuffle=True)\nfor data, target,idxs in tqdm((val_loader)):\n    data = data.to(torch.float)\n    output = model(data.view(data.shape[0],1,*data.shape[1:]).to(device))\n    preds.append((nn.Sigmoid()(output)).detach().cpu().numpy())\n    targs.append(target)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Fscore:\n    def __init__(self):\n        self.epsilon = 1e-7\n        self.tp = 0\n        self.fp = 0\n        self.fn = 0\n    \n    def accumulate(self, y_true, y_pred):\n        # Flatten the input arrays to 1D\n        y_true = np.ravel(y_true)\n        y_pred = np.ravel(y_pred)\n        \n        # Update true positives, false positives, and false negatives\n        self.tp += np.sum((y_true == 1) & (y_pred == 1))\n        self.fp += np.sum((y_true == 0) & (y_pred == 1))\n        self.fn += np.sum((y_true == 1) & (y_pred == 0))\n    \n    def get_score(self, beta=1):\n       # Calculate precision, recall, and F-score\n        precision = self.tp / (self.tp + self.fp + self.epsilon)\n        recall = self.tp / (self.tp + self.fn + self.epsilon)\n        f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + self.epsilon)\n    \n        return f_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\n\ndef round_elements(arr, threshold):\n    rounded_arr = np.array([])\n    for elem in (arr):\n        elem = torch.tensor(elem.reshape(8*8))\n\n        rounded_elem = np.zeros_like(elem)        \n        rounded_arr = np.append(rounded_arr,torch.where(elem >= threshold, torch.ceil(elem), torch.floor(elem)))\n    return rounded_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thrsh = 0.5\n\nscore = Fscore()\nfor p,t in tqdm(zip(preds,targs)):\n    score.accumulate(t,round_elements(p,thrsh))\n\nprint(score.get_score(beta=0.5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flatten = np.concatenate(preds).flatten()\nN= len(flatten)\ntn = N - (score.tp + score.fn + score.fp)\n\nprint(score.tp)\nprint(score.fn)\nprint(score.fp)\n\nprint(score.tp/(score.tp+score.fp))\nprint(score.tp/(score.tp+score.fn))\nprint((score.tp+score.fp)/(tn+score.fn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"picture = np.zeros((8298, 6452))\n\n@numba.njit()\ndef insert_snippet(full_picture,indx_matrix,sheight,swidth,smatrix):\n    height = indx_matrix[0]\n    width = indx_matrix[1]\n    for i in range(sheight):\n        for j in range(swidth):\n            full_picture[height[i],width[j]] = smatrix[i,j]\n    return full_picture\n\npics_loader = DataLoader(data3, batch_size=1)\nfor batch_idx, (x_data, target, indx_arr) in tqdm(enumerate(pics_loader)):\n    x_data = x_data.view(1,*x_data.shape)\n    x_data = x_data.to(torch.float)\n    x_data = torch.tensor((x_data).to(device))\n    pred =  torch.sigmoid(model(x_data))\n    pred_cpu = pred.detach().cpu().numpy()\n    picture = insert_snippet(picture, indx_arr[0, :].T.numpy(), 8,8, pred_cpu.reshape(8, 8))\n    if batch_idx > 400000:\n        break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def round_array_threshold(array, threshold):\n    rounded_array = np.where(array >= threshold, np.ceil(array), np.floor(array))\n    return rounded_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rounded_array = round_array_threshold(picture,0.5)\nplt.imshow(rounded_array)\nplt.colorbar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(score.tp)\nprint(score.fn)\nprint(score.fp)\n\nprint(score.tp/(score.tp+score.fp))\nprint(score.tp/(score.tp+score.fn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((score.tp+score.fp)/(tn+score.fn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flatten = []\nfor p in preds:\n    flatten.append(p.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flatten = np.concatenate(preds).flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N= len(flatten)\ntn = N - (score.tp + score.fn + score.fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(score.tp + tn) / (score.tp + score.fn + score.fp + tn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}